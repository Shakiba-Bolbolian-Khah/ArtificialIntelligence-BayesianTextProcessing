{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Networks Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Artificial Intelligence - CA3\n",
    "> ### Shakiba Bolbolian Khah - 810196426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **About**:\n",
    "\n",
    "Classification using Bayesian networks is one of the useful methods which helps us classify data based on train data. During this assignment, we are trying to classify different type of news using Bayesian network.\n",
    "\n",
    "\n",
    "\n",
    "- **Data Preprocessing**:\n",
    "\n",
    "To use Bayesian network, first we should preprocess raw data and make it ready. This includes several steps.\n",
    "\n",
    "**important**: News are being trained with headers and description! By this way we do not have problem for which does not have description!\n",
    "\n",
    "> **1.Stop Words Omition**:\n",
    "> There is a special group of words which are frequent in all types of news. Since these words, such as am, is, are, they, etc, have negative impact on data analysis, they should be deleted. To do this work we can use `nltk` Python library. It offers a list of standard English stop words.\n",
    ">\n",
    ">**2.Lower Case Alphabet**:\n",
    "> Obviously there is no different between \"word\" and \"Word\", so to consider them two different words, has its inevitable side effect because by this way we decrease the probability of presence of a word. Therefore we convert all capital characters to lower form.\n",
    ">\n",
    ">**3.Ponctuation Marks Omition**:\n",
    "> Ponctutation marks have a little effect on type of news and consiering them makes situation and implementation way difficult with no significant effect. They just can help us find part of speech of words in a sentence. However in this inplementation we do not need part of speech. As a result to omit ponctuation marks will be helpful. This is true about numbers too.\n",
    ">\n",
    ">**4.Lemmatiazation vs Stemming**:\n",
    "> To use words in sentences change them based on their part of speech. Plural form of nouns, adverbs and adjectives with same origins, infinitives, etc are different examples of this kind of change. There are different methods to obtain origins of words, which will be discussed later. So replacing words with thier origins is very important.\n",
    ">\n",
    ">\n",
    ">**5.Tokenization**:\n",
    "> Totally to tokenize sentences is the last preprocess operation we should do.\n",
    "\n",
    "- **Bayesian Rule**:\n",
    "\n",
    "Below came some explanations about Bayesian network and its rule which is used to classify data.\n",
    "\n",
    "> $$P(c|x) = \\frac{P(x|c) P(c)}{P(x)}$$\n",
    ">\n",
    ">\n",
    ">- **$P(c|x)$:** As wanted in problem, it shows that how much is it possible that news have type c if it contains the word x. Clearly it is not easy to calculate. So we use Bayesian rule to obtain it from other probabilities.\n",
    ">\n",
    ">\n",
    ">- **$P(x|c)$:** As it is clear it shows how mush is it possible to have the word like x in news of type c. To explain more, consider the word \"city\". What is the probability of presence of this word in travel news? To calculate this, we first need to have a dictionary to show the number of repeatition of each word in train data of a special type of news. for example the word \"city\" has been repeated in travel news 150 times. Now we need ttal number of words of travel news. Number of city division by total number of words in travel news will give us the probability of having the word city if the type of news is travel. \n",
    ">\n",
    ">\n",
    ">- **$P(c)$:** This refer to probability of being special type of news and can be calculated with division number of train data with category c on total number of train data.\n",
    ">\n",
    ">\n",
    ">- **$P(x)$:** This shows the total probability of presence a word in train data. For instance the probability of having the word \"world\" in train data. Since it is the same in all calculation we do not consider it.\n",
    ">\n",
    ">\n",
    ">\n",
    ">As a result, the final relation which is used to find the prabability of being a special category came below:\n",
    ">\n",
    ">\n",
    ">$$ P(c|x) = P({x}_1|c)P({x}_2|c)\\ldots P({x}_n|c) P(c)$$\n",
    "\n",
    "\n",
    "- **Training Process**:\n",
    "\n",
    "After creating matrix, we need to measure the functionality of model. To do this, we first should have data with correct label to train it and then we need another group of data with correct label to test the model with and check the functionality and performance. It would be not a wise choice to select numbers of data with no attention to its category. So it is better to have a balance between training data category and test data category. To do this, we select a same portion data of each category. For example 80 percent of all data with travel category, 80 percent of business category and so on.\n",
    "\n",
    "- **Date Preprocessing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "BUSINESS = 'BUSINESS'\n",
    "STYLE = 'STYLE & BEAUTY'\n",
    "TRAVEL = 'TRAVEL'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+')\n",
    "\n",
    "def lemmatizing(l):\n",
    "    t = []\n",
    "    for i in l:\n",
    "        t.append(lemmatizer.lemmatize(i))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileReader(path, isTest):\n",
    "    if(isTest):\n",
    "        df = pd.read_csv(path, usecols = ['index','short_description','headline'])\n",
    "    else:\n",
    "        df = pd.read_csv(path, usecols = ['category','index','short_description','headline'])\n",
    "\n",
    "    df['headline'] = df['headline'].str.lower()\n",
    "    df['short_description'] = df['short_description'].str.lower()\n",
    "    df['short_description'] = df['short_description'].fillna(' ')\n",
    "    df['headline'] = df['headline'].fillna(' ')\n",
    "    df['data'] = df['headline'] + ' ' + df['short_description']\n",
    "    df['data'] = df['data'].apply(tokenizer.tokenize)\n",
    "    df['data'] = df['data'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    df['data'] = df['data'].apply(lemmatizing)\n",
    "    return df\n",
    "\n",
    "df = fileReader(\"./Attachment/data.csv\", False)\n",
    "travelBag = df[df['category'] == 'TRAVEL']\n",
    "beautyBag = df[df['category'] == 'STYLE & BEAUTY']\n",
    "businessBag = df[df['category'] == 'BUSINESS']\n",
    "\n",
    "travelIdx =  math.floor(0.8*len(travelBag))\n",
    "trainTravelBag = travelBag[0: travelIdx]\n",
    "evTravelBag = travelBag[travelIdx:]\n",
    "\n",
    "beautyIdx =  math.floor(0.8*len(beautyBag))\n",
    "trainBeautyBag = beautyBag[0: beautyIdx]\n",
    "evBeautyBag = beautyBag[beautyIdx:]\n",
    "\n",
    "businessIdx =  math.floor(0.8*len(businessBag))\n",
    "trainBusinessBag = businessBag[0: businessIdx]\n",
    "evBusinessBag = businessBag[businessIdx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Creating Dictionary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessCount = {}\n",
    "travelCount = {}\n",
    "styleCount = {}\n",
    "\n",
    "def countWords(l, category):\n",
    "    for sen in l:\n",
    "        for word in sen:\n",
    "            if(category == BUSINESS):\n",
    "                businessCount[word] = 2 if word not in businessCount else businessCount[word] + 1\n",
    "                travelCount[word] = 1 if word not in travelCount else travelCount[word]\n",
    "                styleCount[word] = 1 if word not in styleCount else styleCount[word]\n",
    "            elif(category == STYLE):\n",
    "                businessCount[word] = 1 if word not in businessCount else businessCount[word]\n",
    "                travelCount[word] = 1 if word not in travelCount else travelCount[word]\n",
    "                styleCount[word] = 2 if word not in styleCount else styleCount[word] + 1\n",
    "            elif(category == TRAVEL):\n",
    "                businessCount[word] = 1 if word not in businessCount else businessCount[word]\n",
    "                travelCount[word] = 2 if word not in travelCount else travelCount[word] + 1\n",
    "                styleCount[word] = 1 if word not in styleCount else styleCount[word]\n",
    "            \n",
    "            \n",
    "countWords(trainBusinessBag['data'], BUSINESS)\n",
    "countWords(trainTravelBag['data'], TRAVEL)\n",
    "businessNum = sum(businessCount.values())\n",
    "travelNum = sum(travelCount.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Phase I**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In phase I we just train the model with two categories: Business and Travel!\n",
    "To evaluate data it is enough to calculate each category probability and announce the maximum one. To calculate presicion and recall, we need 4 amount:\n",
    ">\n",
    "> 1. number of news which are travel and predicted as travel : trueTravel\n",
    "> 2. number of news which are business and predicted as business : trueBusiness\n",
    "> 3. number of news which are business but predicted as travel : falseTravel\n",
    "> 4. number of news which are travel and predicted as business : falseBusiness\n",
    ">\n",
    "Then it is enough to calculate each value for each categort based on formula, which came below.\n",
    "\n",
    "|Phase1|Travel | Business |\n",
    "|--- | --- | --- |\n",
    "| Recall | 0.9612359550561798 |0.9158091674462114 |\n",
    "| Precision | 0.9500277623542477 |0.9341603053435115 |\n",
    "| Accuracy | 0.9441909441909442 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:    0.9441909441909442\n",
      "\n",
      "Travel\n",
      "Precision:  0.9500277623542477\n",
      "Recall:     0.9612359550561798\n",
      "\n",
      "Business\n",
      "Precision:  0.9341603053435115\n",
      "Recall:     0.9158091674462114\n"
     ]
    }
   ],
   "source": [
    "def findCategoryPhase1(news):\n",
    "    trainNum = (len(trainTravelBag) + len(trainBusinessBag))\n",
    "    travelProb = len(trainTravelBag) / trainNum\n",
    "    businessProb = len(trainBusinessBag) / trainNum\n",
    "    for word in news['data']:\n",
    "        if word in travelCount:\n",
    "            travelProb = travelProb * (travelCount[word] / travelNum)\n",
    "            businessProb = businessProb * (businessCount[word] / businessNum)\n",
    "    return TRAVEL if travelProb > businessProb else BUSINESS\n",
    "\n",
    "def phase1():\n",
    "    trueTravel = 0\n",
    "    falseTravel  = 0\n",
    "    trueBusiness = 0\n",
    "    falseBusiness = 0\n",
    "    for index, row in evTravelBag.iterrows():\n",
    "        if(findCategoryPhase1(row) == TRAVEL):\n",
    "            trueTravel += 1\n",
    "        else:\n",
    "            falseBusiness += 1\n",
    "    for index, row in evBusinessBag.iterrows():\n",
    "        if(findCategoryPhase1(row) == BUSINESS):\n",
    "            trueBusiness += 1\n",
    "        else:\n",
    "            falseTravel += 1\n",
    "            \n",
    "    accuracy = (trueBusiness + trueTravel ) / (trueBusiness + trueTravel + falseBusiness + falseTravel)\n",
    "    print(\"Accuracy:   \", accuracy)\n",
    "    \n",
    "    print(\"\\nTravel\")\n",
    "    precisionTravel = trueTravel / (trueTravel + falseTravel)\n",
    "    recallTravel = trueTravel / (trueTravel + falseBusiness)\n",
    "    print(\"Precision: \", precisionTravel)\n",
    "    print(\"Recall:    \", recallTravel)\n",
    "    \n",
    "    print(\"\\nBusiness\")\n",
    "    precisionBusiness = trueBusiness / (trueBusiness + falseBusiness)\n",
    "    recallBusiness = trueBusiness / (trueBusiness + falseTravel)    \n",
    "    print(\"Precision: \", precisionBusiness)\n",
    "    print(\"Recall:    \", recallBusiness)\n",
    "    \n",
    "phase1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Phase II**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In phase II Style & Beauty added to previous two categories.\n",
    "To evaluate data it is enough to calculate each category probability and announce the maximum one. To calculate presicion and recall, we need 9 amounts according to confusion matrix which are similar to previous phase.\n",
    ">\n",
    "> **Confusion Matrix**:\n",
    ">\n",
    ">The Confusion matrix is one of the most intuitive and easiest metrics used for finding the correctness and accuracy of the model. In this case, it is a matrix with two dimensions: predicted and actual which each one has classes as same as categories. This matrix itself is not a performance metric but it is used to calculate all metrics. Its schema in this problem came below:\n",
    ">> Cells' context are in formatted `x/y`! x shows the actual category and y determines predicted one. In final matrix every cell shows that how many tests were detected as category y but indeed they had category x.\n",
    ">\n",
    ">| || Actual | ||\n",
    "|---| --- | --- | --- | --- |\n",
    "| | | Travel | Business | Style |\n",
    "| |  Travel| T/T | B/T  | S/T |\n",
    "| Predicted | Business | T/B | B/B |  S/B |\n",
    "|  |  Style & Beauty | T/S  | B/S | S/S |\n",
    ">\n",
    ">**Oversampling**:\n",
    ">\n",
    ">Sometimes data in calssification does not have good distribution. Due to this, model may have bios and pretend to predict data with one label more than another. In this example data do not have standard distribution and number of test data with label `Business` is less than others. It has side effects like significant difference between precision and recall and etc.\n",
    ">>\n",
    ">>**Solution**:\n",
    ">> \n",
    ">>- There are different ways to solve this problem and one of them is to sample data with less amount more than others. For instance in this example we can take copy of some data with Business category. By this way we can have more data with category Business and solve the problem. Due to the approach I use in implementation, I did not face this problem so I did not copy data. It is necessary to mention that to copy data is not useful in all cases. The reason is that in this case we are aware of number of train and evaluation too. For example in phase 1 about 33 percent of train data was business category and remained one was travel. From this data we selected 80 percent of each category for training and left 20 percent for evaluation. So evaluation data has similar scale to train data, because of that to copy data does not have significant effect on result. As well as this, copying training data can cause overfitting!\n",
    ">>\n",
    ">>\n",
    ">> - **About my approach** : What happen if we have a word in business category and we do not have it at all in travel? If it comes in a test, probabitlity of being travel news will be 0 because of one word! But what if we have a word which came in travel news but not in business? News containing both of these words niether can be travel nor business, however we know that it is not good at all. To solve it, we can assign a little probability to those words which come in one category and not in others. If we count number of words from 2, problem will be solved. By this way if the word x comes in travel news and does not come in business or style, it has a probability equal to $\\frac{1}{total\\;words}$. Sum of all probability will be 1 too. This can fix some problems too and helps us have better training.\n",
    "\n",
    "\n",
    "A table of needed data came below and confusion matrix is shown in the output too.\n",
    "\n",
    "\n",
    "|Phase1|Travel | Business | Style & Beauty |\n",
    "|--- | --- | --- | --- |\n",
    "| Recall | 0.949438202247191 |0.8877455565949486 | 0.9360967184801382 |\n",
    "| Precision | 0.9194776931447225 |0.9055343511450382 | 0.9564705882352941 |\n",
    "| Accuracy | | 0.930004361098997 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.930004361098997\n",
      "\n",
      "Travel\n",
      "Precision:  0.9194776931447225\n",
      "Recall:     0.949438202247191\n",
      "\n",
      "Business\n",
      "Precision:  0.9055343511450382\n",
      "Recall:     0.8877455565949486\n",
      "\n",
      "Beauty & Style\n",
      "Precision:  0.9564705882352941\n",
      "Recall:     0.9360967184801382\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "countWords(trainBeautyBag['data'], STYLE)\n",
    "styleNum = sum(styleCount.values())\n",
    "businessNum = sum(businessCount.values())\n",
    "travelNum = sum(travelCount.values())\n",
    "\n",
    "def findCategoryPhase2(news):\n",
    "    trainNum = len(trainTravelBag) + len(trainBusinessBag) + len(trainBeautyBag)\n",
    "    travelProb = len(trainTravelBag) / trainNum\n",
    "    businessProb = len(trainBusinessBag) / trainNum\n",
    "    beautyProb = len(beautyBag) / trainNum\n",
    "    for word in news['data']:\n",
    "        if word in travelCount:\n",
    "            travelProb = travelProb * (travelCount[word] / travelNum)\n",
    "            businessProb = businessProb * (businessCount[word] / businessNum)\n",
    "            beautyProb = beautyProb * (styleCount[word] / styleNum)\n",
    "    probs = {travelProb:TRAVEL, businessProb:BUSINESS, beautyProb:STYLE}\n",
    "    return probs[max(probs)]\n",
    "\n",
    "def phase2():\n",
    "    dataMatrix = [[0,0,0],[0,0,0],[0,0,0]]\n",
    "    \n",
    "    for index, row in evTravelBag.iterrows():\n",
    "        tag = findCategoryPhase2(row)\n",
    "        if(tag == TRAVEL):\n",
    "            dataMatrix[0][0] += 1\n",
    "        elif(tag == BUSINESS):\n",
    "            dataMatrix[1][0] += 1\n",
    "        else:\n",
    "            dataMatrix[2][0] += 1\n",
    "            \n",
    "    for index, row in evBusinessBag.iterrows():\n",
    "        tag = findCategoryPhase2(row)\n",
    "        if(tag == BUSINESS):\n",
    "            dataMatrix[1][1] += 1\n",
    "        elif(tag == TRAVEL):\n",
    "            dataMatrix[0][1] += 1\n",
    "        else:\n",
    "            dataMatrix[2][1] += 1\n",
    "      \n",
    "    for index, row in evBeautyBag.iterrows():\n",
    "        tag = findCategoryPhase2(row)\n",
    "        if(tag == STYLE):\n",
    "            dataMatrix[2][2] += 1\n",
    "        elif(tag == TRAVEL):\n",
    "            dataMatrix[0][2] += 1\n",
    "        else:\n",
    "            dataMatrix[1][2] += 1\n",
    "           \n",
    "    labels = [TRAVEL, BUSINESS, STYLE]\n",
    "    df_cm = pd.DataFrame(dataMatrix, index = [i for i in labels],\n",
    "                  columns = [i for i in labels])\n",
    "    plt.figure(figsize = (7,5))\n",
    "    sn.set(font_scale= 0.7)\n",
    "    sn.heatmap(df_cm, annot=True, fmt=\"d\", linewidths=.5)\n",
    "    plt.xlabel('Actual Type')\n",
    "    plt.ylabel('Predicted Type')\n",
    "    \n",
    "    accuracy = (dataMatrix[0][0]+dataMatrix[1][1]+dataMatrix[2][2]) / (sum(dataMatrix[0])+sum(dataMatrix[1])+sum(dataMatrix[2]))\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    \n",
    "    travelPrec = dataMatrix[0][0] / (sum(dataMatrix[0]))\n",
    "    travelRecall = dataMatrix[0][0] / (dataMatrix[0][0]+dataMatrix[1][0]+dataMatrix[2][0])\n",
    "    print(\"\\nTravel\")\n",
    "    print(\"Precision: \", travelPrec)\n",
    "    print(\"Recall:    \", travelRecall)\n",
    "    \n",
    "    businessPrec = dataMatrix[1][1] / (sum(dataMatrix[1]))\n",
    "    businessRecall = dataMatrix[1][1] / (dataMatrix[0][1]+dataMatrix[1][1]+dataMatrix[2][1])\n",
    "    print(\"\\nBusiness\")\n",
    "    print(\"Precision: \", businessPrec)\n",
    "    print(\"Recall:    \", businessRecall)\n",
    "    \n",
    "    stylePrec = dataMatrix[2][2] / (sum(dataMatrix[2]))\n",
    "    styleRecall = dataMatrix[2][2] / (dataMatrix[0][2]+dataMatrix[1][2]+dataMatrix[2][2])\n",
    "    print(\"\\nBeauty & Style\")\n",
    "    print(\"Precision: \", stylePrec)\n",
    "    print(\"Recall:    \", styleRecall)\n",
    "    \n",
    "    \n",
    "phase2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Test Data Classification**:\n",
    "\n",
    "It is similar to previous parts but we do not calculate preformance metrics and just save categories in .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTest(path):\n",
    "    testDf = fileReader(path, True)\n",
    "    output = pd.DataFrame(columns = ['index', 'category'])\n",
    "    for index, row in testDf.iterrows():\n",
    "        tag = findCategoryPhase2(row)\n",
    "        output = output.append({'index':row['index'], 'category':tag}, ignore_index=True)\n",
    "    output.to_csv(r'./output.csv', index = False, header=True)\n",
    "    \n",
    "    \n",
    "fillTest(\"./Attachment/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quesions**\n",
    "\n",
    "**1.** It is better to take a look at the definition of lemmatiazation and stemming. As said before, It is vital to not count different format of words with the same origin and we should consider as one word to have better result. There are two methods to do this which came below.\n",
    ">\n",
    ">**Stemming**: It is not a very smart way. Actually it just cuts the end of words to omit postfixes like ing, ed, s, etc with a hope that it may be correct. So stem function just cuts the word with no word analysis. By this way some words like prties converts to prti with no meaning. However it can be helpful in some cases.\n",
    ">\n",
    ">**Lemmatiazation**: A smarter algorithm which can decide based on part of speech of words in sentences. For example it can change the word better to good if it recieves the part of speech of word as adverb. By this way lemmatiazation can detect words and convert most of them into right format. \n",
    ">\n",
    ">>**Which is better?**: Lemmatiazation is obviously better. It is enough to consider the example of parties. If you have the word party 50 times in your text and parties came 50 times too, by applying stemming you will have 50 'party' and 50 'parti' in your dictionary. However by applying lemmatiazation you will have 100 'party'. In conclusion lemmatiazation is better becauses it prevents data distribution. However it is not easy to use completely. `pos_tag` function in `nltk` can help us find part of speech of each word in sentences. `lemmatize` function in `nltk` just works for main four part of speech : noun, verb, adverb and adjective. On the other hand `pos_tag` returns different parts of speech with huge variaty. So it is not very easy to convert other tag into main one. So in this assignment lemmatizer is used with no part of speech, but is still smarter than stemming.\n",
    ">\n",
    "\n",
    "**2.** Let us explain tf-idf weight.\n",
    ">\n",
    "> **What is tf-idf?** \n",
    ">\n",
    ">tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection . It is often used as a weighting factor in text calssification.\n",
    ">>\n",
    ">> **TF**: TF shows that how much a word is important in a document. It is related to times of appearance in a same document. It is computed as the frequency of a token in a document in bag of words model. Its relation came below.\n",
    ">>\n",
    ">>$$\\mathrm{TF = \\frac{Number\\;of\\;times\\;the\\;token\\;appears\\;in\\;the\\;document}{Total\\;Number\\;of\\;Tokens\\;in\\;the\\;document}}$$\n",
    ">>\n",
    ">> **IDF**: IDF rewards tokens that are rare overall in a dataset. If a rare word occurs in two documents, then it is more important to the meaning of each document. It is computed like below. `t` is a selected token and `U` is set of documents.\n",
    ">>\n",
    ">>$$\\mathrm{IDF(t) = \\frac{Total\\;number\\;of\\;documents}{Number\\;of\\;documents\\;in\\;U\\;that\\;contain\\;t}}$$\n",
    ">>\n",
    ">> **TF-IDF** : At last it will be computed like below.\n",
    ">>\n",
    ">>$$\\mathrm{TF-IDF = TF \\times IDF}$$\n",
    ">>\n",
    ">**How to use tf-idf in Bayesian model?**\n",
    ">\n",
    ">It is enough to consider tf-idf instead of counting number of each appearance of each word. With this implementation, $P(c|x)$ ,in a situation where x is category and c is a word, is computed by this way:\n",
    ">\n",
    ">$$\\mathrm{P(c|x) = \\frac{Number\\;of\\;c\\;in\\;category + 1}{Number\\;of\\;all\\;words+Number\\;of\\;distinct\\;words}}$$\n",
    ">\n",
    "> Now when we want to use tf-idf, we do not need simple counting anymore and we can use tf-idf weight instead which came below.\n",
    ">\n",
    ">$$\\mathrm{P(c|x) = \\frac{tf-idf\\;weight\\;of\\;c + 1}{Sum\\;of\\;all\\;tf-idf\\;weights+Number\\;of\\;distinct\\;words}}$$\n",
    ">\n",
    "\n",
    "**3.** Precision pays attention to proportion of prediction which was correct. This does not pay attention to cases which were not predicted correctly. However recall tells us what proportion of a special category data was predicted correctly too. In recall we pay attention to proportion which were predicted wrong. In conclusion if a system just pay attention to precision, although it shows high number of correct predict, it can cause high number of wrong predict too. For example consider a model which diagnoses cancer. If it has high precision, it shows that among those were predicted as having cancer, high number had actually cancer. In this case we forget all people who had cancer and model did not diagnose so they can die! Recall can help us to find out how many people whose cancer result was predicted false by model had cancer actually and now they are in danger.\n",
    "\n",
    "\n",
    "**4.** As it was described before, if we consider the probability of a word which has come in one category as 0 in others. For example if the word `Tabriz` comes in travel category not others, in this method, probability of being business or style & beauty will be 0 because of one word! Obviously it is not accurate and it is better to consider a little probability for a category on which word has not come at all. As I told, I did this by counting number of repeatition from 2 instead of one. It means that I assume 1 for number of appearance of a word which has never come in a category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
